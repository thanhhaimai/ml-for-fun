{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09557025",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88ccafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.shakespeare_data_source import ShakespeareDataSource\n",
    "from data.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "shakespeare_data_source = ShakespeareDataSource.load(\n",
    "    file_path=\"../../../../datasets/shakespeare/input.txt\",\n",
    ")\n",
    "\n",
    "tokenizer.load(shakespeare_data_source.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac37b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.shakespeare_generator.model import Config\n",
    "\n",
    "\n",
    "config = Config(\n",
    "    batch_size=2**12,\n",
    "    sequence_length=2**6,\n",
    "    embedding_size=2**5,\n",
    "    num_heads=2**3,\n",
    "    num_blocks=2**2,\n",
    "    epochs=1,\n",
    "    dropout=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    patience=30,\n",
    "    min_delta=1e-3,\n",
    "    device=torch.device(\"cuda\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe39551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.shakespeare_generator.shakespeare_dataset import ShakespeareDataset\n",
    "\n",
    "shakespeare_dataset = ShakespeareDataset(\n",
    "    shakespeare_data_source=shakespeare_data_source,\n",
    "    tokenizer=tokenizer,\n",
    "    sequence_length=config.sequence_length,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "\n",
    "print(shakespeare_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.shakespeare_generator.model import ShakespeareGenerator\n",
    "\n",
    "\n",
    "test_model = ShakespeareGenerator(\n",
    "    config=config,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")\n",
    "\n",
    "# shape: [2, 1] -- (B=2, S=1)\n",
    "inputs = torch.ones((2, 1), dtype=torch.long, device=config.device)\n",
    "outputs = test_model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "print(\"---------- Predict from empty input:\")\n",
    "for i in range(outputs.shape[0]):\n",
    "    print(tokenizer.i2t(outputs[i].tolist()))\n",
    "\n",
    "# shape: [1, S]\n",
    "inputs = shakespeare_dataset[0].input.unsqueeze(0).to(config.device, non_blocking=True)\n",
    "outputs = test_model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "print(\"---------- Predict from first sample:\")\n",
    "for i in range(outputs.shape[0]):\n",
    "    print(tokenizer.i2t(outputs[i].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e15fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    shakespeare_dataset,\n",
    "    [0.8, 0.1, 0.1],\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b022eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from learning.shakespeare_generator.model import Batch, ParallelBatchLearner\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "print(config)\n",
    "\n",
    "model = ShakespeareGenerator(\n",
    "    config=config,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "learner = ParallelBatchLearner(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=config.device,\n",
    ")\n",
    "print(learner)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=Batch.from_samples,\n",
    "    num_workers=4,  # Parallel data loading\n",
    "    pin_memory=True,  # Faster CPU->GPU transfer\n",
    "    persistent_workers=True,  # Keep workers alive between epochs\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    collate_fn=Batch.from_samples,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Starting training...\\n\"\n",
    "    f\"Expecting initial loss around {math.log(tokenizer.vocab_size)}\"\n",
    ")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = learner.train(train_dataloader, [])\n",
    "    eval_loss = learner.eval(val_dataloader, [])\n",
    "    print(\n",
    "        f\"{epoch}/{config.epochs} -- {time.time() - start_time:.2f}s \"\n",
    "        f\"\\tTrain loss \\t{train_loss:.4f} \"\n",
    "        f\"\\tEval loss \\t{eval_loss:.4f} \"\n",
    "    )\n",
    "\n",
    "    inputs = torch.ones((1, 1), dtype=torch.long, device=config.device)\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=1000,\n",
    "    )\n",
    "\n",
    "    print(\"---------- Predict from empty input:\")\n",
    "    for i in range(outputs.shape[0]):\n",
    "        print(tokenizer.i2t(outputs[i].tolist()))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Training completed. Elapsed time: {elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.token_to_index)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# shape: [2, 1] -- (B=2, S=1)\n",
    "inputs = torch.ones((1, 1), dtype=torch.long, device=config.device)\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_length=1000,\n",
    ")\n",
    "\n",
    "print(\"---------- Predict from empty input:\")\n",
    "for i in range(outputs.shape[0]):\n",
    "    print(tokenizer.i2t(outputs[i].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "base_path = \"../../../../models\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_save_path = os.path.join(base_path, f\"shakespeare_generator_{timestamp}.pt\")\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "# model_load_path = \"../models/shakespeare_generator.pt\"\n",
    "# model2 = ShakespeareGenerator(\n",
    "#     config=config,\n",
    "#     vocab_size=tokenizer.vocab_size,\n",
    "# )\n",
    "# model2.load_state_dict(torch.load(model_load_path, map_location=config.device))\n",
    "# model2.to(config.device)\n",
    "# model2.eval()\n",
    "# print(f\"Model loaded from {model_load_path}\")\n",
    "\n",
    "# print(model2)\n",
    "\n",
    "# outputs = model2.generate(\n",
    "#     inputs,\n",
    "#     max_length=1000,\n",
    "# )\n",
    "\n",
    "# print(\"---------- Predict from empty input:\")\n",
    "# for i in range(outputs.shape[0]):\n",
    "#     print(tokenizer.i2t(outputs[i].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
