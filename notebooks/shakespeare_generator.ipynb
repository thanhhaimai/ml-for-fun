{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09557025",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ccafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.shakespeare_data_source import ShakespeareDataSource\n",
    "from data.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "shakespeare_data_source = ShakespeareDataSource.load(\n",
    "    file_path=\"../datasets/shakespeare/input.txt\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac37b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.learner import Config\n",
    "\n",
    "\n",
    "config = Config(\n",
    "    batch_size=2**10,\n",
    "    sequence_length=2**3,\n",
    "    embedding_size=2**5,\n",
    "    num_heads=2**2,\n",
    "    epochs=100,\n",
    "    dropout=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    patience=30,\n",
    "    min_delta=1e-3,\n",
    "    device=torch.device(\"cuda\"),\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe39551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.shakespeare_generator.shakespeare_dataset import ShakespeareDataset\n",
    "\n",
    "shakespeare_dataset = ShakespeareDataset(\n",
    "    shakespeare_data_source=shakespeare_data_source,\n",
    "    tokenizer=tokenizer,\n",
    "    sequence_length=config.sequence_length,\n",
    "    device=config.device,\n",
    ")\n",
    "\n",
    "print(shakespeare_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.shakespeare_generator.model import ShakespeareGenerator\n",
    "\n",
    "\n",
    "test_model = ShakespeareGenerator(\n",
    "    config=config,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")\n",
    "\n",
    "# shape: [2, 1] -- (B=2, S=1)\n",
    "inputs = torch.zeros((2, 1), dtype=torch.long, device=config.device)\n",
    "outputs = test_model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "print(\"---------- Predict from empty input:\")\n",
    "for i in range(outputs.shape[0]):\n",
    "    print(tokenizer.i2t(outputs[i].tolist()))\n",
    "\n",
    "# shape: [1, S]\n",
    "inputs = shakespeare_dataset[0].input.unsqueeze(0)\n",
    "outputs = test_model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "print(\"---------- Predict from first sample:\")\n",
    "for i in range(outputs.shape[0]):\n",
    "    print(tokenizer.i2t(outputs[i].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e15fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    shakespeare_dataset,\n",
    "    [0.8, 0.1, 0.1],\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b022eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from learning.shakespeare_generator.model import Batch, ParallelBatchLearner\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "print(config)\n",
    "\n",
    "model = ShakespeareGenerator(\n",
    "    config=config,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "learner = ParallelBatchLearner(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=config.device,\n",
    ")\n",
    "print(learner)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=Batch.from_samples,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    collate_fn=Batch.from_samples,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Starting training...\\n\"\n",
    "    f\"Expecting initial loss around {math.log(tokenizer.vocab_size)}\"\n",
    ")\n",
    "start_time = time.time()\n",
    "learner.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    num_epochs=config.epochs,\n",
    "    patience=config.patience,\n",
    "    min_delta=config.min_delta,\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Training completed. Elapsed time: {elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# shape: [2, 1] -- (B=2, S=1)\n",
    "inputs = torch.zeros((2, 1), dtype=torch.long)\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "print(\"---------- Predict from empty input:\")\n",
    "for i in range(outputs.shape[0]):\n",
    "    print(tokenizer.i2t(outputs[i].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
